==37077== NVPROF is profiling process 37077, command: ./amg2013 -rlx 16 -pooldist 1 -r 32 32 32 -P 1 1 1
==37077== Profiling application: ./amg2013 -rlx 16 -pooldist 1 -r 32 32 32 -P 1 1 1
==37077== Profiling result:
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
 GPU activities:   34.79%  90.143ms      1122  80.340us  1.9520us  450.98us  void ellmv_val<double, bool=1>(int, int, int, int, int const *, double const *, double, double, double const *, double*, int)
                   22.78%  59.011ms       127  464.65us  6.8770us  17.478ms  [CUDA memcpy HtoH]
                   21.58%  55.896ms        41  1.3633ms  3.3930us  25.217ms  void cusparseCsr2Hyb_Kernel<double, int=0>(int, int, int const *, double const *, int const *, int const *, int, int, int*, double*, int*, int*, double*)
                    3.90%  10.115ms       643  15.731us     896ns  3.1778ms  [CUDA memcpy HtoD]
                    3.57%  9.2464ms        34  271.95us  43.232us  702.38us  void stable_sort_by_key_merge_core<int=256, int=4>(int, int*, int*, int*, int*, int*, int*)
                    1.86%  4.8207ms        34  141.79us  5.0240us  408.77us  void stable_sort_by_key_local_core<int=256, int=4>(int, int, int*, int*, int*, int*)
                    1.34%  3.4833ms        45  77.407us  1.4080us  321.06us  void stable_sort_by_key_stop_core<int=256, int=4>(int, int*, int*)
                    1.09%  2.8129ms       154  18.265us  1.7280us  109.15us  cheby_loop2
                    1.02%  2.6491ms         8  331.14us  7.4880us  2.2097ms  grab_diagonals_kernel
                    0.73%  1.8947ms       778  2.4350us  1.2480us  6.8160us  [CUDA memcpy DtoH]
                    0.68%  1.7660ms       154  11.467us  1.5040us  68.385us  cheby_loop1
                    0.66%  1.7225ms       154  11.185us  1.2800us  64.385us  cheby_loop4
                    0.64%  1.6532ms       154  10.735us  1.3440us  64.001us  cheby_loop5
                    0.62%  1.6040ms        66  24.302us  16.704us  36.417us  void dot_kernel<double, double, double, int=128, int=0, int=0>(cublasDotParams<double, double>)
                    0.51%  1.3336ms        29  45.985us  43.424us  47.777us  void axpy_kernel_val<double, double, int=0>(cublasAxpyParamsVal<double, double, double>)
                    0.51%  1.3090ms       100  13.090us  1.3440us  35.008us  void copy_kernel<double, int=0>(cublasCopyParams<double>)
                    0.49%  1.2810ms        13  98.536us  3.6160us  1.1039ms  void CsrToCsc_kernel_build_cscColPtr<double, int=0>(cusparseCsrToCscParams<double>)
                    0.49%  1.2750ms       154  8.2790us  1.2160us  47.457us  cheby_loop3
                    0.48%  1.2471ms       473  2.6360us  1.4400us  6.2400us  create_comm_buffer
                    0.45%  1.1653ms       100  11.652us  3.6160us  40.672us  void stable_sort_by_key_domino_phase1<int=256, int=4>(int, int, int, int*, int*, int*, int*, int*, int*)
                    0.43%  1.1064ms        13  85.104us  2.5920us  906.61us  void CsrToCsc_kernel_build_cscRowInd_cscVal<double, int=1>(cusparseCsrToCscParams<double>)
                    0.42%  1.0882ms        13  83.705us  3.6800us  639.15us  void convert_CsrToCoo_kernel<int=0>(int const *, int, int, int*)
                    0.27%  709.83us        88  8.0660us  6.6880us  15.264us  kernel_SetConstantValue
                    0.15%  382.73us        13  29.440us  2.5920us  294.24us  void CsrToCsc_kernel_copy_and_pset<double>(cusparseCsrToCscParams<double>)
                    0.15%  379.84us        55  6.9060us  6.6880us  7.4240us  kernel_assemble_transpose_result
                    0.10%  255.52us         9  28.391us  28.096us  28.768us  void scal_kernel_val<double, double, int=0>(cublasScalParamsVal<double, double>)
                    0.07%  184.80us       222     832ns     800ns  1.2800us  [CUDA memset]
                    0.07%  181.96us        66  2.7560us  2.4320us  3.5840us  void reduce_1Block_kernel<double, double, double, int=128, int=7>(double*, int, double*)
                    0.06%  146.05us        34  4.2950us  2.9760us  4.8000us  void cusparseIinclusive_scan_domino_v1_core<int=256, int=4>(int, int*, int*, int*, int*, int*)
                    0.05%  130.95us        34  3.8510us  2.7520us  4.3530us  void cusparseIinclusive_localscan_core<int=256, int=4>(int, int*, int*, int*)
                    0.03%  71.907us        34  2.1140us  1.9520us  2.3360us  void cusparseIinclusive_scan_merge_core<int=256, int=4>(int, int, int*, int*, int*)
                    0.00%  8.1280us         6  1.3540us  1.1840us  1.5040us  [CUDA memcpy DtoD]
      API calls:   55.91%  6.03687s       395  15.283ms  226.98us  3.42627s  cudaHostRegister
                   20.01%  2.16054s        70  30.865ms  2.4480us  1.26114s  cudaFree
                   14.38%  1.55263s       393  3.9507ms  188.40us  183.30ms  cudaHostUnregister
                    3.29%  355.09ms       433  820.07us     939ns  91.051ms  cudaMalloc
                    2.75%  296.76ms       132  2.2482ms     917ns  174.81ms  cudaHostAlloc
                    1.08%  117.12ms      3194  36.668us  22.353us  119.50us  cudaLaunch
                    0.63%  68.102ms       241  282.58us     906ns  17.550ms  cudaMemcpy
                    0.57%  61.701ms      1319  46.778us  16.224us  1.5371ms  cudaMemcpyAsync
                    0.35%  37.861ms      1880  20.138us  8.6080us  346.28us  cudaStreamSynchronize
                    0.28%  29.777ms       276  107.89us     578ns  4.8665ms  cuDeviceGetAttribute
                    0.13%  14.138ms     21767     649ns     529ns  19.046us  cudaSetupArgument
                    0.10%  11.241ms      1122  10.018us  8.6950us  22.806us  cudaBindTexture
                    0.08%  9.0709ms      2244  4.0420us  3.2350us  107.29us  cudaPointerGetAttributes
                    0.07%  7.6421ms       349  21.897us  14.378us  97.669us  cudaDeviceSynchronize
                    0.06%  6.9449ms         2  3.4724ms  3.0180ms  3.9269ms  cudaMemGetInfo
                    0.06%  6.5993ms         3  2.1998ms  1.1425ms  3.1352ms  cuDeviceTotalMem
                    0.06%  6.4287ms       222  28.958us  23.335us  283.84us  cudaMemsetAsync
                    0.04%  4.3815ms      1122  3.9050us  3.3510us  5.0120us  cudaUnbindTexture
                    0.04%  4.2900ms      5698     752ns     544ns  4.4710us  cudaGetLastError
                    0.03%  3.6436ms         3  1.2145ms  515.99us  2.2978ms  cuDeviceGetName
                    0.02%  2.6565ms      3194     831ns     589ns  9.2430us  cudaConfigureCall
                    0.01%  1.5384ms        92  16.721us  14.961us  24.747us  cudaFuncGetAttributes
                    0.00%  503.65us        66  7.6310us  7.0720us  13.652us  cudaEventQuery
                    0.00%  478.36us       126  3.7960us  2.8360us  13.689us  cudaHostGetDevicePointer
                    0.00%  425.98us         2  212.99us  76.775us  349.20us  cudaStreamCreate
                    0.00%  309.08us        66  4.6830us  4.1740us  8.5780us  cudaEventRecord
                    0.00%  110.14us        32  3.4410us  2.9440us  9.1600us  cudaFuncSetAttribute
                    0.00%  55.552us        16  3.4720us  3.0640us  6.5950us  cudaEventCreateWithFlags
                    0.00%  44.259us        21  2.1070us  1.5310us  5.7580us  cudaDeviceGetAttribute
                    0.00%  25.685us         2  12.842us  4.0140us  21.671us  cudaSetDevice
                    0.00%  15.360us         2  7.6800us  5.4830us  9.8770us  cudaGetDevice
                    0.00%  8.2430us         5  1.6480us     813ns  4.3390us  cuDeviceGetCount
                    0.00%  4.4630us         2  2.2310us     840ns  3.6230us  cudaGetDeviceCount
                    0.00%  4.3530us         4  1.0880us     845ns  1.5590us  cuDeviceGet
                    0.00%  4.1600us         2  2.0800us  1.6110us  2.5490us  cuInit
                    0.00%  3.0000us         2  1.5000us  1.3690us  1.6310us  cuDriverGetVersion

==37077== NVTX result:
==37077==   Thread "<unnamed>" (id = 309424)
==37077==     Domain "<unnamed>"
==37077==       Range "MPI_Allreduce"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  99.485ms       234  425.15us  7.6810us  4.4453ms  MPI_Allreduce
No kernels were profiled in this range.
No API activities were profiled in this range.

==37077==       Range "MPI_Irecv"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  1.9381ms       630  3.0760us  1.1430us  62.524us  MPI_Irecv
No kernels were profiled in this range.
No API activities were profiled in this range.

==37077==       Range "MPI_Isend"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  7.4613ms       630  11.843us  1.0470us  1.5148ms  MPI_Isend
No kernels were profiled in this range.
No API activities were profiled in this range.

==37077==       Range "MPI_Waitall"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  303.70ms       863  351.91us  1.5610us  72.225ms  MPI_Waitall
No kernels were profiled in this range.
No API activities were profiled in this range.

==37077==       Range "hypre_ParCSRMatrixMatvec"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  473.64ms       599  790.72us  12.005us  15.703ms  hypre_ParCSRMatrixMatvec
 GPU activities:   94.99%  78.167ms       979  79.844us  1.9520us  320.68us  void ellmv_val<double, bool=1>(int, int, int, int, int const *, double const *, double, double, double const *, double*, int)
                    1.79%  1.4700ms       473  3.1070us  1.2480us  6.8160us  [CUDA memcpy DtoH]
                    1.71%  1.4051ms       496  2.8320us     896ns  10.433us  [CUDA memcpy HtoD]
                    1.52%  1.2471ms       473  2.6360us  1.4400us  6.2400us  create_comm_buffer
      API calls:   66.84%  67.167ms      1452  46.258us  25.388us  73.377us  cudaLaunch
                   32.78%  32.938ms       957  34.417us  19.490us  105.88us  cudaMemcpyAsync
                    0.38%  386.10us        12  32.174us  28.304us  45.895us  cudaMemcpy

==37077==       Range "hypre_ParCSRMatrixMatvecT"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  31.186ms        77  405.01us  110.97us  3.4225ms  hypre_ParCSRMatrixMatvecT
 GPU activities:   95.53%  11.975ms       143  83.742us  3.6160us  450.98us  void ellmv_val<double, bool=1>(int, int, int, int, int const *, double const *, double, double, double const *, double*, int)
                    3.03%  379.84us        55  6.9060us  6.6880us  7.4240us  kernel_assemble_transpose_result
                    0.89%  111.17us        66  1.6840us  1.2480us  2.8480us  [CUDA memcpy DtoH]
                    0.56%  69.792us        60  1.1630us     896ns  1.7600us  [CUDA memcpy HtoD]
      API calls:   70.56%  7.9781ms       198  40.293us  25.515us  59.259us  cudaLaunch
                   15.04%  1.7003ms        60  28.337us  24.150us  35.502us  cudaMemcpy
                   14.40%  1.6285ms        66  24.673us  23.018us  33.622us  cudaMemcpyAsync

==37077==       Range "hypre_ParCSRRelax_Cheby"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  3.18387s       165  19.296ms  3.4933ms  296.48ms  hypre_ParCSRRelax_Cheby
 GPU activities:   81.35%  52.605ms       616  85.397us  7.6800us  320.68us  void ellmv_val<double, bool=1>(int, int, int, int, int const *, double const *, double, double, double const *, double*, int)
                    4.35%  2.8129ms       154  18.265us  1.7280us  109.15us  cheby_loop2
                    2.73%  1.7660ms       154  11.467us  1.5040us  68.385us  cheby_loop1
                    2.66%  1.7225ms       154  11.185us  1.2800us  64.385us  cheby_loop4
                    2.56%  1.6532ms       154  10.735us  1.3440us  64.001us  cheby_loop5
                    1.97%  1.2750ms       154  8.2790us  1.2160us  47.457us  cheby_loop3
                    1.73%  1.1199ms       315  3.5550us     896ns  272.71us  [CUDA memcpy HtoD]
                    1.47%  950.96us       308  3.0870us  1.2800us  6.8160us  [CUDA memcpy DtoH]
                    1.18%  761.52us       308  2.4720us  1.4400us  6.2400us  create_comm_buffer
      API calls:   74.51%  63.742ms      1694  37.627us  23.632us  73.256us  cudaLaunch
                   25.27%  21.619ms       617  35.038us  19.490us  105.88us  cudaMemcpyAsync
                    0.22%  192.08us         6  32.012us  28.304us  38.077us  cudaMemcpy

==37077==       Range "hypre_SeqVectorAxpy"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  8.8344ms       109  81.049us  3.3240us  590.77us  hypre_SeqVectorAxpy
 GPU activities:  100.00%  1.3336ms        29  45.985us  43.424us  47.777us  void axpy_kernel_val<double, double, int=0>(cublasAxpyParamsVal<double, double, double>)
      API calls:  100.00%  823.22us        29  28.386us  25.132us  37.532us  cudaLaunch

==37077==       Range "hypre_SeqVectorCopy"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  13.845ms       188  73.644us  3.2420us  903.67us  hypre_SeqVectorCopy
 GPU activities:  100.00%  1.3090ms       100  13.090us  1.3440us  35.008us  void copy_kernel<double, int=0>(cublasCopyParams<double>)
      API calls:  100.00%  2.7619ms       100  27.618us  24.501us  36.840us  cudaLaunch

==37077==       Range "hypre_SeqVectorInnerProd"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  23.085ms       234  98.653us  4.5540us  599.85us  hypre_SeqVectorInnerProd
 GPU activities:   85.71%  1.6040ms        66  24.302us  16.704us  36.417us  void dot_kernel<double, double, double, int=128, int=0, int=0>(cublasDotParams<double, double>)
                    9.72%  181.96us        66  2.7560us  2.4320us  3.5840us  void reduce_1Block_kernel<double, double, double, int=128, int=7>(double*, int, double*)
                    4.57%  85.569us        66  1.2960us  1.2480us  1.5680us  [CUDA memcpy DtoH]
      API calls:   61.40%  3.5169ms       132  26.642us  23.425us  119.50us  cudaLaunch
                   38.60%  2.2113ms        66  33.503us  30.585us  41.262us  cudaMemcpyAsync

==37077==       Range "hypre_SeqVectorScale"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  393.27us         9  43.697us  39.549us  68.639us  hypre_SeqVectorScale
 GPU activities:  100.00%  255.52us         9  28.391us  28.096us  28.768us  void scal_kernel_val<double, double, int=0>(cublasScalParamsVal<double, double>)
      API calls:  100.00%  268.40us         9  29.821us  27.037us  43.243us  cudaLaunch

==37077==       Range "hypre_SeqVectorSetConstantValues"
            Type  Time(%)      Time     Calls       Avg       Min       Max  Name
          Range:  100.00%  3.7014ms        88  42.061us  39.911us  50.591us  hypre_SeqVectorSetConstantValues
 GPU activities:  100.00%  709.83us        88  8.0660us  6.6880us  15.264us  kernel_SetConstantValue
      API calls:  100.00%  2.6841ms        88  30.500us  29.071us  34.030us  cudaLaunch

